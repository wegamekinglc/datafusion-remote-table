use crate::connection::{big_decimal_to_i128, projections_contains};
use crate::transform::transform_batch;
use crate::{
    Connection, ConnectionOptions, DFResult, OracleType, Pool, RemoteField, RemoteSchema,
    RemoteSchemaRef, RemoteType, Transform,
};
use bb8_oracle::OracleConnectionManager;
use datafusion::arrow::array::{
    ArrayRef, BinaryBuilder, BooleanBuilder, Date64Builder, Decimal128Builder, Float32Builder,
    Float64Builder, Int16Builder, Int32Builder, LargeBinaryBuilder, LargeStringBuilder,
    RecordBatch, StringBuilder, TimestampNanosecondBuilder, TimestampSecondBuilder, make_builder,
};
use datafusion::arrow::datatypes::{DataType, SchemaRef, TimeUnit};
use datafusion::common::{DataFusionError, project_schema};
use datafusion::execution::SendableRecordBatchStream;
use datafusion::physical_plan::stream::RecordBatchStreamAdapter;
use derive_getters::Getters;
use derive_with::With;
use futures::StreamExt;
use oracle::sql_type::OracleType as ColumnType;
use oracle::{Connector, Row};
use std::sync::Arc;

#[derive(Debug, Clone, With, Getters)]
pub struct OracleConnectionOptions {
    pub(crate) host: String,
    pub(crate) port: u16,
    pub(crate) username: String,
    pub(crate) password: String,
    pub(crate) service_name: String,
    pub(crate) chunk_size: Option<usize>,
}

impl OracleConnectionOptions {
    pub fn new(
        host: impl Into<String>,
        port: u16,
        username: impl Into<String>,
        password: impl Into<String>,
        service_name: impl Into<String>,
    ) -> Self {
        Self {
            host: host.into(),
            port,
            username: username.into(),
            password: password.into(),
            service_name: service_name.into(),
            chunk_size: None,
        }
    }
}

#[derive(Debug)]
pub struct OraclePool {
    pool: bb8::Pool<OracleConnectionManager>,
}

pub(crate) async fn connect_oracle(options: &OracleConnectionOptions) -> DFResult<OraclePool> {
    let connect_string = format!(
        "//{}:{}/{}",
        options.host, options.port, options.service_name
    );
    let connector = Connector::new(
        options.username.clone(),
        options.password.clone(),
        connect_string,
    );
    let _ = connector
        .connect()
        .map_err(|e| DataFusionError::Internal(format!("Failed to connect to oracle: {e:?}")))?;
    let manager = OracleConnectionManager::from_connector(connector);
    let pool = bb8::Pool::builder()
        .build(manager)
        .await
        .map_err(|e| DataFusionError::Internal(format!("Failed to create oracle pool: {:?}", e)))?;
    Ok(OraclePool { pool })
}

#[async_trait::async_trait]
impl Pool for OraclePool {
    async fn get(&self) -> DFResult<Arc<dyn Connection>> {
        let conn = self.pool.get_owned().await.map_err(|e| {
            DataFusionError::Execution(format!("Failed to get oracle connection due to {e:?}"))
        })?;
        Ok(Arc::new(OracleConnection { conn }))
    }
}

#[derive(Debug)]
pub struct OracleConnection {
    conn: bb8::PooledConnection<'static, OracleConnectionManager>,
}

#[async_trait::async_trait]
impl Connection for OracleConnection {
    async fn infer_schema(
        &self,
        sql: &str,
        transform: Option<Arc<dyn Transform>>,
    ) -> DFResult<(RemoteSchemaRef, SchemaRef)> {
        let row = self.conn.query_row(sql, &[]).map_err(|e| {
            DataFusionError::Execution(format!("Failed to query one row to infer schema: {e:?}"))
        })?;
        let remote_schema = Arc::new(build_remote_schema(&row)?);
        let arrow_schema = Arc::new(remote_schema.to_arrow_schema());
        if let Some(transform) = transform {
            let batch = rows_to_batch(&[row], &arrow_schema, None)?;
            let transformed_batch = transform_batch(
                batch,
                transform.as_ref(),
                &arrow_schema,
                None,
                Some(&remote_schema),
            )?;
            Ok((remote_schema, transformed_batch.schema()))
        } else {
            Ok((remote_schema, arrow_schema))
        }
    }

    async fn query(
        &self,
        conn_options: &ConnectionOptions,
        sql: &str,
        table_schema: SchemaRef,
        projection: Option<&Vec<usize>>,
    ) -> DFResult<SendableRecordBatchStream> {
        let projected_schema = project_schema(&table_schema, projection)?;
        let projection = projection.cloned();
        let chunk_size = conn_options.chunk_size();
        let result_set = self.conn.query(sql, &[]).map_err(|e| {
            DataFusionError::Execution(format!("Failed to execute query on oracle: {e:?}"))
        })?;
        let stream = futures::stream::iter(result_set)
            .chunks(chunk_size.unwrap_or(2048))
            .boxed();

        let stream = stream.map(move |rows| {
            let rows: Vec<Row> = rows
                .into_iter()
                .collect::<Result<Vec<_>, _>>()
                .map_err(|e| {
                    DataFusionError::Execution(format!(
                        "Failed to collect rows from oracle due to {e}",
                    ))
                })?;
            rows_to_batch(rows.as_slice(), &table_schema, projection.as_ref())
        });

        Ok(Box::pin(RecordBatchStreamAdapter::new(
            projected_schema,
            stream,
        )))
    }
}

fn oracle_type_to_remote_type(oracle_type: &ColumnType) -> DFResult<RemoteType> {
    match oracle_type {
        ColumnType::Number(precision, scale) => {
            // TODO need more investigation on the precision and scale
            let precision = if *precision == 0 { 38 } else { *precision };
            let scale = if *scale == -127 { 0 } else { *scale };
            Ok(RemoteType::Oracle(OracleType::Number(precision, scale)))
        }
        ColumnType::BinaryFloat => Ok(RemoteType::Oracle(OracleType::BinaryFloat)),
        ColumnType::BinaryDouble => Ok(RemoteType::Oracle(OracleType::BinaryDouble)),
        ColumnType::Float(precision) => Ok(RemoteType::Oracle(OracleType::Float(*precision))),
        ColumnType::Varchar2(size) => Ok(RemoteType::Oracle(OracleType::Varchar2(*size))),
        ColumnType::NVarchar2(size) => Ok(RemoteType::Oracle(OracleType::NVarchar2(*size))),
        ColumnType::Char(size) => Ok(RemoteType::Oracle(OracleType::Char(*size))),
        ColumnType::NChar(size) => Ok(RemoteType::Oracle(OracleType::NChar(*size))),
        ColumnType::Long => Ok(RemoteType::Oracle(OracleType::Long)),
        ColumnType::CLOB => Ok(RemoteType::Oracle(OracleType::Clob)),
        ColumnType::NCLOB => Ok(RemoteType::Oracle(OracleType::NClob)),
        ColumnType::Raw(size) => Ok(RemoteType::Oracle(OracleType::Raw(*size))),
        ColumnType::LongRaw => Ok(RemoteType::Oracle(OracleType::LongRaw)),
        ColumnType::BLOB => Ok(RemoteType::Oracle(OracleType::Blob)),
        ColumnType::Date => Ok(RemoteType::Oracle(OracleType::Date)),
        ColumnType::Timestamp(_) => Ok(RemoteType::Oracle(OracleType::Timestamp)),
        ColumnType::Boolean => Ok(RemoteType::Oracle(OracleType::Boolean)),
        _ => Err(DataFusionError::NotImplemented(format!(
            "Unsupported oracle type: {oracle_type:?}",
        ))),
    }
}

fn build_remote_schema(row: &Row) -> DFResult<RemoteSchema> {
    let mut remote_fields = vec![];
    for col in row.column_info() {
        let remote_type = oracle_type_to_remote_type(col.oracle_type())?;
        remote_fields.push(RemoteField::new(col.name(), remote_type, col.nullable()));
    }
    Ok(RemoteSchema::new(remote_fields))
}

macro_rules! handle_primitive_type {
    ($builder:expr, $field:expr, $col:expr, $builder_ty:ty, $value_ty:ty, $row:expr, $index:expr, $convert:expr) => {{
        let builder = $builder
            .as_any_mut()
            .downcast_mut::<$builder_ty>()
            .unwrap_or_else(|| {
                panic!(
                    "Failed to downcast builder to {} for {:?} and {:?}",
                    stringify!($builder_ty),
                    $field,
                    $col
                )
            });
        let v = $row.get::<usize, Option<$value_ty>>($index).map_err(|e| {
            DataFusionError::Execution(format!(
                "Failed to get {} value for {:?} and {:?}: {e:?}",
                stringify!($value_ty),
                $field,
                $col
            ))
        })?;

        match v {
            Some(v) => builder.append_value($convert(v)?),
            None => builder.append_null(),
        }
    }};
}

fn rows_to_batch(
    rows: &[Row],
    table_schema: &SchemaRef,
    projection: Option<&Vec<usize>>,
) -> DFResult<RecordBatch> {
    let projected_schema = project_schema(table_schema, projection)?;
    let mut array_builders = vec![];
    for field in table_schema.fields() {
        let builder = make_builder(field.data_type(), rows.len());
        array_builders.push(builder);
    }

    for row in rows {
        for (idx, field) in table_schema.fields.iter().enumerate() {
            if !projections_contains(projection, idx) {
                continue;
            }
            let builder = &mut array_builders[idx];
            let col = row.column_info().get(idx);
            match field.data_type() {
                DataType::Int16 => {
                    handle_primitive_type!(builder, field, col, Int16Builder, i16, row, idx, |v| {
                        Ok::<_, DataFusionError>(v)
                    });
                }
                DataType::Int32 => {
                    handle_primitive_type!(builder, field, col, Int32Builder, i32, row, idx, |v| {
                        Ok::<_, DataFusionError>(v)
                    });
                }
                DataType::Float32 => {
                    handle_primitive_type!(
                        builder,
                        field,
                        col,
                        Float32Builder,
                        f32,
                        row,
                        idx,
                        |v| { Ok::<_, DataFusionError>(v) }
                    );
                }
                DataType::Float64 => {
                    handle_primitive_type!(
                        builder,
                        field,
                        col,
                        Float64Builder,
                        f64,
                        row,
                        idx,
                        |v| { Ok::<_, DataFusionError>(v) }
                    );
                }
                DataType::Utf8 => {
                    handle_primitive_type!(
                        builder,
                        field,
                        col,
                        StringBuilder,
                        String,
                        row,
                        idx,
                        |v| { Ok::<_, DataFusionError>(v) }
                    );
                }
                DataType::LargeUtf8 => {
                    handle_primitive_type!(
                        builder,
                        field,
                        col,
                        LargeStringBuilder,
                        String,
                        row,
                        idx,
                        |v| { Ok::<_, DataFusionError>(v) }
                    );
                }
                DataType::Decimal128(_precision, scale) => {
                    handle_primitive_type!(
                        builder,
                        field,
                        col,
                        Decimal128Builder,
                        String,
                        row,
                        idx,
                        |v: String| {
                            let decimal = v.parse::<bigdecimal::BigDecimal>().map_err(|e| {
                                DataFusionError::Execution(format!(
                                    "Failed to parse BigDecimal from {v:?}: {e:?}",
                                ))
                            })?;
                            big_decimal_to_i128(&decimal, Some(*scale as i32)).ok_or_else(|| {
                                DataFusionError::Execution(format!(
                                    "Failed to convert BigDecimal to i128 for {decimal:?}",
                                ))
                            })
                        }
                    );
                }
                DataType::Timestamp(TimeUnit::Second, None) => {
                    handle_primitive_type!(
                        builder,
                        field,
                        col,
                        TimestampSecondBuilder,
                        chrono::NaiveDateTime,
                        row,
                        idx,
                        |v: chrono::NaiveDateTime| {
                            let t = v.and_utc().timestamp();
                            Ok::<_, DataFusionError>(t)
                        }
                    );
                }
                DataType::Timestamp(TimeUnit::Nanosecond, None) => {
                    handle_primitive_type!(
                        builder,
                        field,
                        col,
                        TimestampNanosecondBuilder,
                        chrono::NaiveDateTime,
                        row,
                        idx,
                        |v: chrono::NaiveDateTime| {
                            v.and_utc().timestamp_nanos_opt().ok_or_else(|| {
                                DataFusionError::Execution(format!(
                                    "Failed to convert chrono::NaiveDateTime {v} to nanos timestamp"
                                ))
                            })
                        }
                    );
                }
                DataType::Date64 => {
                    handle_primitive_type!(
                        builder,
                        field,
                        col,
                        Date64Builder,
                        chrono::NaiveDateTime,
                        row,
                        idx,
                        |v: chrono::NaiveDateTime| {
                            Ok::<_, DataFusionError>(v.and_utc().timestamp_millis())
                        }
                    );
                }
                DataType::Boolean => {
                    handle_primitive_type!(
                        builder,
                        field,
                        col,
                        BooleanBuilder,
                        bool,
                        row,
                        idx,
                        |v| { Ok::<_, DataFusionError>(v) }
                    );
                }
                DataType::Binary => {
                    handle_primitive_type!(
                        builder,
                        field,
                        col,
                        BinaryBuilder,
                        Vec<u8>,
                        row,
                        idx,
                        |v| { Ok::<_, DataFusionError>(v) }
                    );
                }
                DataType::LargeBinary => {
                    handle_primitive_type!(
                        builder,
                        field,
                        col,
                        LargeBinaryBuilder,
                        Vec<u8>,
                        row,
                        idx,
                        |v| { Ok::<_, DataFusionError>(v) }
                    );
                }
                _ => {
                    return Err(DataFusionError::NotImplemented(format!(
                        "Unsupported data type {:?} for col: {:?}",
                        field.data_type(),
                        col
                    )));
                }
            }
        }
    }

    let projected_columns = array_builders
        .into_iter()
        .enumerate()
        .filter(|(idx, _)| projections_contains(projection, *idx))
        .map(|(_, mut builder)| builder.finish())
        .collect::<Vec<ArrayRef>>();
    Ok(RecordBatch::try_new(projected_schema, projected_columns)?)
}
